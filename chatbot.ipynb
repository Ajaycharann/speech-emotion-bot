{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNUtIPrsXqmfwE0gB15aRAS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajaycharann/speech-emotion-bot/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrBP6piS74rg",
        "outputId": "cceeb702-8d69-41f5-b25a-a2977ceb8740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda, model: google/flan-t5-small\n",
            "Loading tokenizer and model (this can take 30-120s)...\n",
            "ðŸ”‘ No ngrok auth token provided in the script.\n",
            "31XPr7BozgpdVgvctxA3v76uHvy_3P9abZMLufBX5YwCdJptH31XPr7BozgpdVgvctxA3v76uHvy_3P9abZMLufBX5YwCdJptH\n",
            "Starting ngrok tunnel on port 8000...\n",
            "ðŸš€ Public API URL (paste this into the React app): https://de3a791eab9b.ngrok-free.app/chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [560]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     183.82.97.138:0 - \"OPTIONS /chat HTTP/1.1\" 200 OK\n",
            "INFO:     183.82.97.138:0 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "INFO:     183.82.97.138:0 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "INFO:     183.82.97.138:0 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "INFO:     183.82.97.138:0 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "INFO:     183.82.97.138:0 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "INFO:     183.82.97.138:0 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "INFO:     183.82.97.138:0 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "INFO:     183.82.97.138:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "# Colab cell: FastAPI LLM backend + ngrok\n",
        "# Run this in Google Colab. After model loads it will print a public URL like:\n",
        "# https://abc123.ngrok-free.app/chat\n",
        "# Copy that URL (append /chat) and paste into the React app.\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q fastapi uvicorn[standard] pyngrok transformers accelerate nest_asyncio torch --upgrade\n",
        "\n",
        "# Optional: install bitsandbytes if you want 4/8-bit loading (may help for big models)\n",
        "# !pip install -q bitsandbytes\n",
        "\n",
        "# --- Backend code ---\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "# Replace with a small/medium model that fits Colab GPU.\n",
        "# If you have a GPU runtime, device will be \"cuda\", else \"cpu\".\n",
        "# Recommended starter models that usually fit Colab GPU:\n",
        "#  - \"google/flan-t5-small\"  (small, CPU/GPU friendly, seq2seq style)\n",
        "#  - \"tiiuae/falcon-7b-instruct\" (7B - may not fit on free Colab GPUs)\n",
        "# Modify MODEL_ID if you want to try another model.\n",
        "MODEL_ID = \"google/flan-t5-small\"\n",
        "USE_SEQ2SEQ = True  # flan-t5 is seq2seq; set True for T5-like models\n",
        "\n",
        "# ngrok token: either set here or the cell will ask you to paste it\n",
        "NGROK_AUTH_TOKEN = \"\"  # <-- paste your ngrok authtoken here if you want\n",
        "\n",
        "# ---------- Setup device ----------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}, model: {MODEL_ID}\")\n",
        "\n",
        "# ---------- Load model ----------\n",
        "print(\"Loading tokenizer and model (this can take 30-120s)...\")\n",
        "if USE_SEQ2SEQ:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID)\n",
        "else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
        "\n",
        "# Move model to device\n",
        "try:\n",
        "    model = model.to(device)\n",
        "except Exception as e:\n",
        "    print(\"Model to(device) failed:\", e)\n",
        "\n",
        "# ---------- Create FastAPI app ----------\n",
        "app = FastAPI(title=\"Colab LLM Bridge\")\n",
        "\n",
        "# Allow CORS from localhost:3000 and any origin (ngrok + local dev)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # relaxed for dev; restrict for production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "class Message(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.get(\"/\")\n",
        "def index():\n",
        "    return {\"status\": \"ok\", \"model\": MODEL_ID, \"device\": device}\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "def chat(msg: Message):\n",
        "    # The model type matters: seq2seq (T5/Flan) vs causal (GPT-style).\n",
        "    prompt = msg.text.strip()\n",
        "    if not prompt:\n",
        "        return {\"reply\": \"\"}\n",
        "\n",
        "    # Add a short system instruction if you'd like\n",
        "    # For seq2seq we just feed the prompt; for causal we may prepend instruction.\n",
        "    if USE_SEQ2SEQ:\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        outputs = model.generate(**input_ids, max_new_tokens=200, do_sample=True, top_p=0.95, temperature=0.7)\n",
        "        reply = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        outputs = model.generate(input_ids[\"input_ids\"], max_new_tokens=200, do_sample=True, top_p=0.95, temperature=0.7)\n",
        "        reply = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return {\"reply\": reply}\n",
        "\n",
        "# ---------- Expose via ngrok ----------\n",
        "if not NGROK_AUTH_TOKEN:\n",
        "    print(\"ðŸ”‘ No ngrok auth token provided in the script.\")\n",
        "    NGROK_AUTH_TOKEN = input(\"31XPr7BozgpdVgvctxA3v76uHvy_3P9abZMLufBX5YwCdJptH\").strip()\n",
        "\n",
        "# Configure ngrok\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "print(\"Starting ngrok tunnel on port 8000...\")\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(\"ðŸš€ Public API URL (paste this into the React app):\", public_url + \"/chat\")\n",
        "\n",
        "# ---------- Run server ----------\n",
        "# Allow nested event loop (Colab)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    }
  ]
}